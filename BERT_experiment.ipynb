{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "\n",
    "import jax.numpy as torch\n",
    "import scipy\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "from tucker_riemopt.tucker import Tucker\n",
    "from tucker_riemopt import backend as back\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from src.TuckerLinear import TuckerLinear, TuckerLinearPermute\n",
    "from src.utils import optim\n",
    "from src.utils.riemann_model import RiemannModel, RiemannParameter\n",
    "from tucker_riemopt import set_backend\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "back.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from copy import deepcopy\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "model_name = \"fabriceyhc/bert-base-uncased-imdb\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_orig = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_with_rank(c = 10, n_layers=50):\n",
    "    model = deepcopy(model_orig)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    old_params = 0\n",
    "    new_params = 0\n",
    "    linears = [k.split('.') for k, m in model.named_modules() if type(m).__name__ == 'Linear']\n",
    "    \n",
    "    linears = linears[-1 - n_layers: -1]\n",
    "    \n",
    "    # for linear in tqdm(linears):\n",
    "    for linear in linears:\n",
    "        code = 'model'\n",
    "        for l in linear:\n",
    "            if l.isdigit():\n",
    "                code += f'[{l}]'\n",
    "            else:\n",
    "                code += f'.{l}'\n",
    "        layer = eval(code)\n",
    "        if (layer.in_features, layer.out_features) == (768, 768):\n",
    "            dims1 = [8, 8, 12]\n",
    "            dims2 = [8, 8, 12]\n",
    "            # dims1 = [4, 4, 4, 4, 3]\n",
    "            # dims2 = [4, 4, 4, 4, 3]\n",
    "            rank = [c, c, c]\n",
    "        elif (layer.in_features, layer.out_features) == (768, 3072):\n",
    "            dims1 = [8, 8, 12]\n",
    "            dims2 = [16, 16, 12]\n",
    "            rank = [c, c, c]\n",
    "        elif (layer.in_features, layer.out_features) == (3072, 768):\n",
    "            dims1 = [16, 16, 12]\n",
    "            dims2 = [8, 8, 12]\n",
    "            rank = [c, c, c]\n",
    "        elif (layer.in_features, layer.out_features) == (768, 2):\n",
    "            dims1 = [8, 8, 12]\n",
    "            dims2 = [2, 1, 1]\n",
    "            rank = [c, c, c]\n",
    "        else:\n",
    "            print(\"ERROR\", (layer.in_features, layer.out_features))\n",
    "            assert 1 == 0\n",
    "        \n",
    "        old_params += sum(p.numel() for p in layer.parameters())\n",
    "        exec(code + f' = TuckerLinearPermute({layer.in_features}, {layer.out_features}, layer=layer, rank=rank, dims1=dims1, dims2=dims2)')\n",
    "        new_params += sum(p.numel() for p in eval(code + '.riemann_parameters()'))\n",
    "    \n",
    "    print(old_params)\n",
    "    print(new_params)\n",
    "    return model, n_params / (n_params - old_params + new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return { 'Accuracy': np.mean(predictions == labels) }\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tucker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_model(model, test_loader):\n",
    "    model.eval()\n",
    "    cum_loss = torch.zeros((1,), device=device)\n",
    "    acc = torch.zeros((1,), device=device)\n",
    "    # for batch in tqdm(test_loader):\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        output = model(**batch)\n",
    "        pred = output.logits.argmax(dim=-1).detach()\n",
    "        # print(pred.shape)\n",
    "        # print(batch['labels'].shape)\n",
    "        acc += (pred == batch['labels']).float().sum()\n",
    "        loss = criterion(output.logits, batch['labels'])\n",
    "        # loss = output.loss\n",
    "        cum_loss += loss.detach()\n",
    "    return cum_loss.item() / len(test_loader), acc.item() / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "def fine_tune_epoch(model, train_loader, riem_optimizer, criterion):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        riem_optimizer.zero_grad()\n",
    "        output = model(**batch)\n",
    "        loss = criterion(output.logits, batch['labels'])\n",
    "        # loss = output.loss\n",
    "        if (i + 1) % 10 == 0:\n",
    "            wandb.log({'train_loss': loss})\n",
    "            pass\n",
    "        loss.backward()\n",
    "        riem_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_riemann_optimizer(model):\n",
    "    tucker_linears = [k.split('.') for k, m in model.named_modules() if type(m).__name__ == 'TuckerLinearPermute']\n",
    "    riem_params = []\n",
    "\n",
    "    for tl in tucker_linears:\n",
    "        code = 'model'\n",
    "        for l in tl:\n",
    "            if l.isdigit():\n",
    "                code += f'[{l}]'\n",
    "            else:\n",
    "                code += f'.{l}'\n",
    "        layer = eval(code)\n",
    "        riem_params += [{\n",
    "            \"params\": layer.riemann_parameters(),\n",
    "            \"rank\": layer.rank\n",
    "        }]\n",
    "\n",
    "    riem_params = riem_params[:-1]\n",
    "\n",
    "    # for p in new_model.regular_parameters():\n",
    "    #     p.requires_grad_(False)\n",
    "    riemann_opt = optim.SGDmomentum(riem_params, base_lr=1e-1)\n",
    "    return riemann_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 40, 210])\n"
     ]
    }
   ],
   "source": [
    "from src.TuckerLinear import TuckerLinearPermute\n",
    "import torch\n",
    "from torch import nn\n",
    "from tucker_riemopt import backend as back\n",
    "\n",
    "back.set_backend('pytorch')\n",
    "\n",
    "# get_model_with_rank(5)\n",
    "layer = TuckerLinearPermute(24, 210, layer=nn.Linear(24, 210), rank=[3, 3, 3], dims1=[2, 3, 4], dims2=[5, 6, 7], bias=False)\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "A = torch.randn(100, 40, 24).to(device)\n",
    "result = layer(A)\n",
    "\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD, AdamW, Adam\n",
    "import wandb\n",
    "\n",
    "num_model_params = sum(p.numel() for p in model_orig.parameters())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(small_eval_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "cs = [1, 2, 3, 5]\n",
    "nlrs = [10, 20, 30, 50]\n",
    "\n",
    "f = open(\"log_usual.txt\", \"a\")\n",
    "\n",
    "print(\"C\\tn_layers\\tmethod\\tloss\\tacc\\tcompression\")\n",
    "print(\"C\\tn_layers\\tmethod\\tloss\\tacc\\tcompression\", file=f)\n",
    "\n",
    "f.close()\n",
    "\n",
    "for c in cs:\n",
    "    for nlr in nlrs:\n",
    "        # model = deepcopy(model_orig)\n",
    "        \n",
    "        f = open(\"log_usual.txt\", \"a\")\n",
    "\n",
    "        model, compression = get_model_with_rank(c=c, n_layers=nlr)\n",
    "        \n",
    "        tucker_linears = [k.split('.') for k, m in model.named_modules() if type(m).__name__ == 'TuckerLinearPermute']\n",
    "        riem_params = []\n",
    "        \n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        for tl in tucker_linears:\n",
    "            code = 'model'\n",
    "            for l in tl:\n",
    "                if l.isdigit():\n",
    "                    code += f'[{l}]'\n",
    "                else:\n",
    "                    code += f'.{l}'\n",
    "            layer = eval(code)\n",
    "            \n",
    "            for p in layer.riemann_parameters():\n",
    "                p.requires_grad_(True)\n",
    "            \n",
    "            riem_params += [{\n",
    "                \"params\": layer.riemann_parameters(),\n",
    "                \"rank\": layer.rank\n",
    "            }]\n",
    "        \n",
    "        # Use for usual SGD\n",
    "        # riemann_opt = SGD(riem_params, lr=1e-3, momentum=0.9)\n",
    "        \n",
    "        # Use for riemopt\n",
    "        riemann_opt = optim.SGDmomentum(riem_params, base_lr=1e-3)\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        eval_loss, eval_acc = eval_model(model, test_loader)\n",
    "        print(c, nlr, \"replace\", round(eval_loss, 3), round(eval_acc, 3), round(compression, 3), sep='\\t')\n",
    "        print(c, nlr, \"replace\", round(eval_loss, 3), round(eval_acc, 3), round(compression, 3), sep='\\t', file=f)\n",
    "        \n",
    "        wandb.init(project=\"rieman-neural-nets\")\n",
    "\n",
    "        fine_tune_epoch(model, train_loader, riemann_opt, criterion)\n",
    "        eval_loss, eval_acc = eval_model(model, test_loader)\n",
    "        num_model_params = sum(p.numel() for p in model_orig.parameters())\n",
    "        n_params = sum(p.numel() for p in model.parameters())\n",
    "        print(c, nlr, \"finetune\", round(eval_loss, 3), round(eval_acc, 3), round(compression, 3), sep='\\t')\n",
    "        print(c, nlr, \"finetune\", round(eval_loss, 3), round(eval_acc, 3), round(compression, 3), sep='\\t', file=f)\n",
    "        \n",
    "        f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SF-Tucker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symmetric_model_with_rank(c = 10, n_layers=50):\n",
    "    model = deepcopy(model_orig)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    old_params = 0\n",
    "    new_params = 0\n",
    "    linears = [k.split('.') for k, m in model.named_modules() if type(m).__name__ == 'Linear']\n",
    "    \n",
    "    linears = linears[-1 - n_layers: -1]\n",
    "    \n",
    "    for linear in tqdm(linears):\n",
    "    # for linear in linears:\n",
    "        code = 'model'\n",
    "        for l in linear:\n",
    "            if l.isdigit():\n",
    "                code += f'[{l}]'\n",
    "            else:\n",
    "                code += f'.{l}'\n",
    "        layer = eval(code)\n",
    "        if (layer.in_features, layer.out_features) == (768, 768):\n",
    "            dims1 = [8, 8, 12]\n",
    "            dims2 = [8, 8, 12]\n",
    "            # dims1 = [4, 4, 4, 4, 3]\n",
    "            # dims2 = [4, 4, 4, 4, 3]\n",
    "            rank = [c, c, c]\n",
    "        elif (layer.in_features, layer.out_features) == (768, 3072):\n",
    "            dims1 = [8, 8, 12]\n",
    "            dims2 = [16, 16, 12]\n",
    "            rank = [c, c, c]\n",
    "        elif (layer.in_features, layer.out_features) == (3072, 768):\n",
    "            dims1 = [16, 16, 12]\n",
    "            dims2 = [8, 8, 12]\n",
    "            rank = [c, c, c]\n",
    "        elif (layer.in_features, layer.out_features) == (768, 2):\n",
    "            dims1 = [8, 8, 12]\n",
    "            dims2 = [2, 1, 1]\n",
    "            rank = [c, c, c]\n",
    "        else:\n",
    "            print(\"ERROR\", (layer.in_features, layer.out_features))\n",
    "            assert 1 == 0\n",
    "        \n",
    "        old_params += sum(p.numel() for p in layer.parameters())\n",
    "        exec(code + f' = TuckerLinearSymmetric({layer.in_features}, {layer.out_features}, layer=layer, rank=rank, dims1=dims1, dims2=dims2)')\n",
    "        new_params += sum(p.numel() for p in eval(code + '.riemann_parameters()'))\n",
    "    \n",
    "    print(old_params)\n",
    "    print(new_params)\n",
    "    return model, n_params / (n_params - old_params + new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_model(model, test_loader):\n",
    "    model.eval()\n",
    "    cum_loss = torch.zeros((1,), device=device)\n",
    "    acc = torch.zeros((1,), device=device)\n",
    "    for batch in tqdm(test_loader):\n",
    "    # for batch in test_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        output = model(**batch)\n",
    "        pred = output.logits.argmax(dim=-1).detach()\n",
    "        # print(pred.shape)\n",
    "        # print(batch['labels'].shape)\n",
    "        acc += (pred == batch['labels']).float().sum()\n",
    "        loss = criterion(output.logits, batch['labels'])\n",
    "        # loss = output.loss\n",
    "        cum_loss += loss.detach()\n",
    "    return cum_loss.item() / len(test_loader), acc.item() / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "def fine_tune_epoch(model, train_loader, riem_optimizer, criterion):\n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        riem_optimizer.zero_grad()\n",
    "        output = model(**batch)\n",
    "        loss = criterion(output.logits, batch['labels'])\n",
    "        # loss = output.loss\n",
    "        sum_loss += loss\n",
    "        if (i + 1) % 10 == 0:\n",
    "            wandb.log({'train_loss': loss})\n",
    "            # print(sum_loss / 10)\n",
    "            sum_loss = 0\n",
    "            pass\n",
    "        loss.backward()\n",
    "        riem_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src.TuckerLinear import TuckerLinearPermute, TuckerLinearSymmetric\n",
    "import wandb\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(small_eval_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model, compression = get_symmetric_model_with_rank(c=1, n_layers=10)\n",
    "eval_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tucker_riemopt.symmetric.optim import SGDmomentum as SGDmomentumSym\n",
    "from src.utils import optim\n",
    "import wandb\n",
    "\n",
    "num_model_params = sum(p.numel() for p in model_orig.parameters())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(small_eval_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "cs = [1, 2, 3, 5]\n",
    "\n",
    "nlrs = [10, 20, 30, 50]\n",
    "\n",
    "f = open(\"log_sym.txt\", \"a\")\n",
    "\n",
    "print(\"C\\tn_layers\\tmethod\\tloss\\tacc\\tcompression\")\n",
    "print(\"C\\tn_layers\\tmethod\\tloss\\tacc\\tcompression\", file=f)\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n",
    "for c in cs:\n",
    "    for nlr in nlrs:\n",
    "        # model = deepcopy(model_orig)\n",
    "        \n",
    "        f = open(\"log_sym.txt\", \"a\")\n",
    "\n",
    "        model, compression = get_symmetric_model_with_rank(c=c, n_layers=nlr)\n",
    "        \n",
    "        tucker_linears = [k.split('.') for k, m in model.named_modules() if type(m).__name__ == 'TuckerLinearSymmetric']\n",
    "        riem_params = []\n",
    "        \n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        for tl in tucker_linears:\n",
    "            code = 'model'\n",
    "            for l in tl:\n",
    "                if l.isdigit():\n",
    "                    code += f'[{l}]'\n",
    "                else:\n",
    "                    code += f'.{l}'\n",
    "            layer = eval(code)\n",
    "            \n",
    "            for p in layer.riemann_parameters():\n",
    "                p.requires_grad_(True)\n",
    "            \n",
    "            riem_params += [{\n",
    "                \"params\": layer.riemann_parameters(),\n",
    "                \"rank\": layer.rank\n",
    "            }]\n",
    "            \n",
    "        riemann_opt = SGDmomentumSym(riem_params, max_lr=1e-4)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        eval_loss, eval_acc = eval_model(model, test_loader)\n",
    "        print(c, nlr, \"replace\", round(eval_loss, 3), round(eval_acc, 3), round(compression, 3), sep='\\t')\n",
    "        print(c, nlr, \"replace\", round(eval_loss, 3), round(eval_acc, 3), round(compression, 3), sep='\\t', file=f)\n",
    "        \n",
    "        wandb.init(project=\"rieman-neural-nets\")\n",
    "\n",
    "        fine_tune_epoch(model, train_loader, riemann_opt, criterion)\n",
    "        eval_loss, eval_acc = eval_model(model, test_loader)\n",
    "        num_model_params = sum(p.numel() for p in model_orig.parameters())\n",
    "        n_params = sum(p.numel() for p in model.parameters())\n",
    "        print(c, nlr, \"finetune\", round(eval_loss, 3), round(eval_acc, 3), round(compression, 3), sep='\\t')\n",
    "        print(c, nlr, \"finetune\", round(eval_loss, 3), round(eval_acc, 3), round(compression, 3), sep='\\t', file=f)\n",
    "        \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
